D:\ProgramData\Anaconda3\envs\HEDL\python.exe D:/allensll/github/citation/lstm.py
Epoch:  0
train loss: 17.36179247946347
train loss: 17.29281575804725
train loss: 16.838750715296694
train loss: 16.38772078943162
train loss: 15.77730892985458
train loss: 14.462679278483579
train loss: 13.416812992674577
train loss: 13.631313986346255
train loss: 12.793837462835876
train loss: 12.152822361581899
train loss: 10.9532443944467
train loss: 19.16095910764924
train loss: 10.315127964444981
train loss: 10.130297462517651
train loss: 10.546262851732644
train loss: 9.846332063766813
train loss: 9.609337447694672
train loss: 9.477970386509943
train loss: 9.379373883957873
train loss: 9.360937856145656
test loss: 9.247399046674374
Epoch:  1
train loss: 9.573293393812598
train loss: 8.911189451400274
train loss: 8.714582125278083
train loss: 8.639087690552358
train loss: 8.56644429286178
train loss: 8.510905964396484
train loss: 8.422575326901466
train loss: 8.588421236056217
train loss: 8.262232166725072
train loss: 8.190843633766448
train loss: 8.087009845148
train loss: 8.032725434163064
train loss: 8.055391979308475
train loss: 7.970267252431312
train loss: 7.9517643933594755
train loss: 8.237412127308513
train loss: 7.9208781246075715
train loss: 7.906432310402911
train loss: 7.87948647311324
train loss: 7.8518444891165515
test loss: 7.3189732948644695
Epoch:  2
train loss: 7.828728042314126
train loss: 7.772852138821207
train loss: 7.736269266513754
train loss: 7.700117787259171
train loss: 7.652964195526219
train loss: 7.630286711445425
train loss: 7.623292063016466
train loss: 7.611862313794218
train loss: 7.5972465096620505
train loss: 7.580295605168167
train loss: 7.568502878368801
train loss: 7.554739177333948
train loss: 7.5479517746254725
train loss: 7.543168398947387
train loss: 7.529563353941745
train loss: 7.5168699962891665
train loss: 7.501055227433622
train loss: 7.480599819718793
train loss: 7.464803108558913
train loss: 7.452048538975579
test loss: 6.85337752889473
Epoch:  3
train loss: 7.447687999995486
train loss: 7.434883191215031
train loss: 7.43141757682586
train loss: 7.428677419648389
train loss: 7.41946318745311
train loss: 7.416163438682525
train loss: 7.407329696705104
train loss: 7.402359776531294
train loss: 7.3983025831768
train loss: 7.394559805426628
train loss: 7.39336689350097
train loss: 7.392580785556275
train loss: 7.390609321580691
train loss: 7.3870804618104815
train loss: 7.379265587871263
train loss: 7.371934107669663
train loss: 7.368087776706717
train loss: 7.3654097272426045
train loss: 7.361881174159833
train loss: 7.357081548735257
test loss: 6.7178718052680315
Epoch:  4
train loss: 7.352342380557875
train loss: 7.346629150156491
train loss: 7.340385541578953
train loss: 7.334886124946557
train loss: 7.3290196705916815
train loss: 7.325311684580586
train loss: 7.32348728046607
train loss: 7.320753684608882
train loss: 7.31752334651629
train loss: 7.313512344553429
train loss: 7.309927037728719
train loss: 7.305698187968234
train loss: 7.302296160125653
train loss: 7.2999005951894915
train loss: 7.298538030060165
train loss: 7.297280908001705
train loss: 7.296158178522232
train loss: 7.294841032018438
train loss: 7.292547307772743
train loss: 7.287892008029604
test loss: 6.660773958054098
Epoch:  5
train loss: 7.281343923409643
train loss: 7.2783159206188195
train loss: 7.276031058526382
train loss: 7.274123050603958
train loss: 7.271573102916321
train loss: 7.267811038007196
train loss: 7.26548949664822
train loss: 7.263593477228057
train loss: 7.262119494514261
train loss: 7.259076898844447
train loss: 7.257313548684613
train loss: 7.255704100007773
train loss: 7.254254398221617
train loss: 7.252733775288054
train loss: 7.249426314987976
train loss: 7.242500855145633
train loss: 7.238803609812959
train loss: 7.234425669617155
train loss: 7.232884922939265
train loss: 7.231466071734725
test loss: 6.617081029957423
Epoch:  6
train loss: 7.230425746821192
train loss: 7.228772626121043
train loss: 7.225239057663693
train loss: 7.2220811359281365
train loss: 7.219147625339203
train loss: 7.225515950553645
train loss: 7.211936130094227
train loss: 7.209509873768824
train loss: 7.203748896498298
train loss: 7.201725576994769
train loss: 7.200281281268187
train loss: 7.198817654274016
train loss: 7.1981387037610505
train loss: 7.197357738542904
train loss: 7.196199464459605
train loss: 7.193594888445303
train loss: 7.190345643557008
train loss: 7.18694293733286
train loss: 7.185033167540067
train loss: 7.1834744851479835
test loss: 6.560628519372266
Epoch:  7
train loss: 7.182967665084269
train loss: 7.182497395636159
train loss: 7.18172225544558
train loss: 7.180117239996328
train loss: 7.177819796602793
train loss: 7.175873766832951
train loss: 7.175344378716844
train loss: 7.174453836228063
train loss: 7.1727234919285845
train loss: 7.171500563459389
train loss: 7.170524325984851
train loss: 7.169471412531422
train loss: 7.1677865156140745
train loss: 7.166650516624894
train loss: 7.165018106934015
train loss: 7.164021693282716
train loss: 7.163200117267373
train loss: 7.162291328113419
train loss: 7.160542781685664
train loss: 7.159502776970606
test loss: 6.539027258860891
Epoch:  8
train loss: 7.15867929471807
train loss: 7.158042310460182
train loss: 7.157252944661994
train loss: 7.156425064260273
train loss: 7.154820924705023
train loss: 7.153025633385825
train loss: 7.151567628561894
train loss: 7.1508017356820135
train loss: 7.149390704253069
train loss: 7.148843363527041
train loss: 7.148425583027876
train loss: 7.147893847151412
train loss: 7.1471993239291365
train loss: 7.14658131984138
train loss: 7.1457947347396855
train loss: 7.144802926652917
train loss: 7.143243305629242
train loss: 7.142153337551655
train loss: 7.14082488152373
train loss: 7.139992236332774
test loss: 6.517699057073206
Epoch:  9
train loss: 7.1394121815018625
train loss: 7.138917777264171
train loss: 7.138495559456048
train loss: 7.138211759237074
train loss: 7.137891917590986
train loss: 7.137300570422631
train loss: 7.1363183095160485
train loss: 7.134006090114719
train loss: 7.130749754553792
train loss: 7.129001208042139
train loss: 7.132526282007824
train loss: 7.128241278325138
train loss: 7.13383777296591
train loss: 7.123755072035473
train loss: 7.123019935978177
train loss: 7.122320070870863
train loss: 7.121571008625245
train loss: 7.121357332841503
train loss: 7.1211474734602405
train loss: 7.12088791040816
test loss: 6.497587089229322

Process finished with exit code 0